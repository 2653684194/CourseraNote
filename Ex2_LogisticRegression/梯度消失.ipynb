{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd858a65",
   "metadata": {},
   "source": [
    "### 什么是Sigmoid函数？\n",
    "\n",
    "首先，我们需要了解Sigmoid函数是什么。Sigmoid函数是一种常用的激活函数，特别是在早期的神经网络中。它的数学表达式如下：\n",
    "\n",
    "$\\sigma(x) = \\frac{1}{1 + e^{-x}}$\n",
    "\n",
    "这个函数的输出范围在0到1之间，形状类似于字母\"S\"，因此得名Sigmoid（S形）。它可以将任何实数输入映射到(0,1)区间，这使得它在二分类问题中非常有用，可以解释为概率。\n",
    "\n",
    "### Sigmoid函数的导数\n",
    "\n",
    "为了理解梯度消失问题，我们需要看看Sigmoid函数的导数。Sigmoid函数的导数可以通过微积分求得：\n",
    "\n",
    "$\\sigma'(x) = \\sigma(x) \\cdot (1 - \\sigma(x))$\n",
    "\n",
    "这是因为：\n",
    "\n",
    "$\\sigma(x) = \\frac{1}{1 + e^{-x}} = (1 + e^{-x})^{-1}$\n",
    "\n",
    "求导：\n",
    "\n",
    "$\\sigma'(x) = -1 \\cdot (1 + e^{-x})^{-2} \\cdot (-e^{-x}) = \\frac{e^{-x}}{(1 + e^{-x})^2}$\n",
    "\n",
    "可以将其重写为：\n",
    "\n",
    "$\\sigma'(x) = \\frac{1}{1 + e^{-x}} \\cdot \\frac{e^{-x}}{1 + e^{-x}} = \\sigma(x) \\cdot (1 - \\sigma(x))$\n",
    "\n",
    "因为：\n",
    "\n",
    "$\\frac{e^{-x}}{1 + e^{-x}} = \\frac{1 + e^{-x} - 1}{1 + e^{-x}} = 1 - \\frac{1}{1 + e^{-x}} = 1 - \\sigma(x)$\n",
    "\n",
    "### Sigmoid函数导数的性质\n",
    "\n",
    "观察Sigmoid的导数：\n",
    "\n",
    "$\\sigma'(x) = \\sigma(x) \\cdot (1 - \\sigma(x))$\n",
    "\n",
    "由于 $\\sigma(x)$ 的输出在(0,1)之间，所以：\n",
    "\n",
    "- 当 $\\sigma(x)$ 接近0或1时， $\\sigma'(x)$ 接近0。\n",
    "  - 当\\(x\\)很大（正或负）， $\\sigma(x)$接近1或0，导数接近0。\n",
    "-  $\\sigma'(x)$ 的最大值出现在 $\\sigma(x) = 0.5$ 时，此时 $\\sigma'(x) = 0.5 \\times 0.5 = 0.25$。\n",
    "\n",
    "因此，Sigmoid函数的导数的最大值是0.25，且随着输入绝对值增大，导数迅速趋近于0。\n",
    "\n",
    "### 梯度消失问题\n",
    "\n",
    "在神经网络中，我们使用反向传播算法来计算梯度，即损失函数对各个参数的偏导数。反向传播通过链式法则将梯度从输出层向输入层传播。对于多层网络，梯度是许多导数的乘积。\n",
    "\n",
    "假设我们有一个多层网络，每一层都使用Sigmoid激活函数。在反向传播时，梯度可以表示为：\n",
    "\n",
    " $\\frac{\\partial L}{\\partial w_1} = \\frac{\\partial L}{\\partial f_n} \\cdot \\frac{\\partial f_n}{\\partial f_{n-1}} \\cdot \\ldots \\cdot \\frac{\\partial f_2}{\\partial f_1} \\cdot \\frac{\\partial f_1}{\\partial w_1}$\n",
    "\n",
    "其中， $\\frac{\\partial f_{i+1}}{\\partial f_i}$ 包含Sigmoid的导数 $\\sigma'\\$。\n",
    "\n",
    "由于Sigmoid的导数最大为0.25，且在许多情况下更小，多个这样的导数相乘会迅速导致梯度指数级减小。例如：\n",
    "\n",
    " $0.25 \\times 0.25 \\times 0.25 \\times \\ldots$\n",
    "\n",
    "对于深度网络（层数很多），这些小的梯度相乘会使得初始层的梯度几乎为零。这意味着：\n",
    "\n",
    "- 初始层的权重几乎不更新（因为梯度接近于零）。\n",
    "- 网络难以学习初始层的特征，训练变得非常缓慢或停止。\n",
    "\n",
    "这种现象被称为“梯度消失”（Vanishing Gradient）。\n",
    "\n",
    "### 梯度消失的影响\n",
    "\n",
    "梯度消失会导致：\n",
    "\n",
    "1. **深层网络训练困难**：初始层的参数几乎不更新，网络无法有效利用深度。\n",
    "2. **训练速度慢**：梯度小，参数更新步长小，收敛缓慢。\n",
    "3. **性能受限**：网络无法学习到更深层次的特征表示。\n",
    "\n",
    "### 为什么Sigmoid容易导致梯度消失？\n",
    "\n",
    "1. **导数范围小**：Sigmoid的导数最大为0.25，且多数情况下更小。\n",
    "2. **链式法则的累乘**：多层小导数的乘积迅速趋近于零。\n",
    "3. **饱和性**：当输入绝对值较大时，Sigmoid输出接近0或1，导数接近零，称为“饱和”。饱和时梯度几乎为零，参数难以更新。\n",
    "\n",
    "### 与其他激活函数的比较\n",
    "\n",
    "现代神经网络更多使用ReLU（Rectified Linear Unit）及其变体作为激活函数，部分原因是为了缓解梯度消失问题。ReLU的定义为：\n",
    "\n",
    " $\\text{ReLU}(x) = \\max(0, x)$\n",
    "\n",
    "其导数为：\n",
    "\n",
    " $\\text{ReLU}'(x) = \\begin{cases} 1 & \\text{if } x > 0 \\\\ 0 & \\text{if } x \\leq 0 \\end{cases}$\n",
    "\n",
    "ReLU的优点：\n",
    "\n",
    "- 正区间的导数为1，避免了梯度消失（在正激活时）。\n",
    "- 计算简单，加速训练。\n",
    "\n",
    "但ReLU也有“死亡ReLU”问题（即某些神经元永远不激活）。\n",
    "\n",
    "### 如何缓解Sigmoid的梯度消失？\n",
    "\n",
    "虽然Sigmoid本身容易导致梯度消失，但可以采取以下措施缓解：\n",
    "\n",
    "1. **使用其他激活函数**：如ReLU、Leaky ReLU、ELU等。\n",
    "2. **权重初始化**：如Xavier初始化，适应Sigmoid的特性。\n",
    "3. **批归一化（Batch Normalization）**：保持激活值的分布，防止饱和。\n",
    "4. **残差连接（ResNet）**：通过跳跃连接避免梯度消失。\n",
    "5. **梯度裁剪**：防止梯度爆炸时连带影响。\n",
    "\n",
    "### 示例说明\n",
    "\n",
    "假设一个简单的三层网络，每层使用Sigmoid激活。反向传播时：\n",
    "\n",
    " $\\frac{\\partial L}{\\partial w_1} = \\frac{\\partial L}{\\partial a_3} \\cdot \\sigma'(z_3) \\cdot w_3 \\cdot \\sigma'(z_2) \\cdot w_2 \\cdot \\sigma'(z_1) \\cdot x$\n",
    "\n",
    "如果 $\\sigma'(z_i)$ 都很小（如0.1），则：\n",
    "\n",
    " $\\frac{\\partial L}{\\partial w_1} \\approx \\text{（其他项）} \\times 0.1 \\times 0.1 \\times 0.1 = \\text{（其他项）} \\times 0.001$\n",
    "\n",
    "梯度迅速减小。\n",
    "\n",
    "### 数学推导\n",
    "\n",
    "设有一个神经元：\n",
    "\n",
    " $a = \\sigma(z), \\quad z = w x + b$\n",
    "\n",
    "损失函数\\(L\\)对\\(w\\)的梯度：\n",
    "\n",
    " $\\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial a} \\cdot \\frac{\\partial a}{\\partial z} \\cdot \\frac{\\partial z}{\\partial w} = \\frac{\\partial L}{\\partial a} \\cdot \\sigma'(z) \\cdot x$\n",
    "\n",
    "对于多层：\n",
    "\n",
    " $\\frac{\\partial L}{\\partial w_1} = \\frac{\\partial L}{\\partial a_n} \\cdot \\prod_{i=2}^{n} \\left( w_i \\cdot \\sigma'(z_{i-1}) \\right) \\cdot \\sigma'(z_1) \\cdot x$\n",
    "\n",
    "如果 $\\sigma'(z_i)$ 和 $\\\\w_i$ 较小，乘积会指数级减小。\n",
    "\n",
    "### 可视化\n",
    "\n",
    "绘制Sigmoid及其导数：\n",
    "\n",
    "- Sigmoid：从0平滑过渡到1。\n",
    "- Sigmoid'：钟形曲线，峰值0.25，两侧快速趋近0。\n",
    "\n",
    "当输入\\(x\\)的绝对值大时，导数接近0。\n",
    "\n",
    "### 总结\n",
    "\n",
    "**Sigmoid的梯度消失**指的是在深度神经网络中，由于Sigmoid函数的导数在多数情况下取值较小（最大0.25），且在反向传播时需要多层导数连续相乘，导致梯度指数级减小，初始层的权重几乎无法更新，从而使得深层网络训练困难的现象。这是Sigmoid函数在深度网络中被逐渐替代的主要原因之一。现代神经网络通常采用ReLU等激活函数来缓解这一问题。"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
