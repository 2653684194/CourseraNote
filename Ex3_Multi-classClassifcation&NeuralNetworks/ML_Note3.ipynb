{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b59d0c2a",
   "metadata": {},
   "source": [
    "# **多类分类和神经网络——手写数字识别**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a575e53c",
   "metadata": {},
   "source": [
    "# One-vs-All Logistic Regression\n",
    "\n",
    "One-vs-All (OvA), also called One-vs-Rest (OvR), is a strategy for extending binary classification algorithms like logistic regression to multi-class classification problems.\n",
    "\n",
    "## How It Works\n",
    "\n",
    "1. **For each class**, a binary classifier is trained to distinguish that class from all other classes combined\n",
    "2. This creates **K separate binary classifiers** for a K-class problem\n",
    "3. During prediction:\n",
    "   - Each classifier outputs a probability score for its class\n",
    "   - The class with the highest probability is selected as the final prediction\n",
    "\n",
    "## Mathematical Formulation\n",
    "\n",
    "For each class k (out of K total classes), we train a logistic regression model:\n",
    "\n",
    "P(y = k | x) = σ(wₖᵀx + bₖ)\n",
    "\n",
    "Where:\n",
    "- σ is the sigmoid function: σ(z) = 1/(1 + e⁻ᶻ)\n",
    "- wₖ is the weight vector for class k\n",
    "- bₖ is the bias term for class k\n",
    "\n",
    "\n",
    "## Advantages\n",
    "\n",
    "- Simple to implement and understand\n",
    "- Works well when the number of classes is not too large\n",
    "- Can use any binary classification algorithm as the base classifier\n",
    "\n",
    "## Disadvantages\n",
    "\n",
    "- Can be computationally expensive for many classes (requires training K models)\n",
    "- May suffer from class imbalance when one class is much smaller than others\n",
    "- Ignores potential relationships between classes\n",
    "\n",
    "## When to Use\n",
    "\n",
    "- When your problem has a moderate number of classes (typically < 10)\n",
    "- When base classifiers perform well on binary tasks\n",
    "- When you need a simple, interpretable multi-class solution\n",
    "\n",
    "Many machine learning libraries (like scikit-learn) implement this strategy automatically when you use their logistic regression implementation for multi-class problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "dce55fac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__header__': b'MATLAB 5.0 MAT-file, Platform: GLNXA64, Created on: Sun Oct 16 13:09:09 2011',\n",
       " '__version__': '1.0',\n",
       " '__globals__': [],\n",
       " 'X': array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]),\n",
       " 'y': array([[10],\n",
       "        [10],\n",
       "        [10],\n",
       "        ...,\n",
       "        [ 9],\n",
       "        [ 9],\n",
       "        [ 9]], dtype=uint8)}"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "\n",
    "data = loadmat('ex3data1.mat')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "04456b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z:np.ndarray) -> np.ndarray:\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7d8cb4",
   "metadata": {},
   "source": [
    "**正则化代价函数**：\n",
    "$$J\\left(\\vec{w},b\\right)=-\\frac{1}{m}\\sum_{i=1}^{m}\\left[y^{\\left(i\\right)}\\log{\\left(f_{\\vec{w},b}\\left(\\vec{x^{\\left(i\\right)}}\\right)\\right)}+\\left(1-y^{\\left(i\\right)}\\right)\\log{\\left(1-f_{\\vec{w},b}\\left(\\vec{x^{\\left(i\\right)}}\\right)\\right)}\\right]+\\frac{\\lambda}{2m}\\sum_{j=1}^{n}w_j^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "9e585044",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Regularized_CostFunction(theta: np.ndarray, X: np.ndarray, outcome: np.ndarray, Lambda: float) -> float:\n",
    "\n",
    "    theta = np.array(theta).reshape(-1, 1)\n",
    "    X = np.array(X)\n",
    "    outcome = np.array(outcome).reshape(-1, 1)\n",
    "    \n",
    "    m = outcome.shape[0]  # Number of training examples\n",
    "    z = X @ theta\n",
    "    h = sigmoid(z)\n",
    "\n",
    "    cost = - np.sum(outcome.T @ np.log(h) + (1 - outcome).T @ np.log(1 - h)) / m\n",
    "    reg = (Lambda / (2 * m)) * np.sum(theta[1:]**2)  # Skip theta[0] for regularization\n",
    "\n",
    "    return float(cost + reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "695257e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularized Cost: 160.39425758157174\n"
     ]
    }
   ],
   "source": [
    "# 训练模型时候转换y为0-1\n",
    "def RegularizedCostFunction(theta:np.ndarray, X:np.ndarray, y:np.ndarray, lambda_:float) -> float:\n",
    "    m = len(y)\n",
    "    theta = theta.reshape(-1, 1)\n",
    "    y = y.reshape(-1, 1)\n",
    "\n",
    "    h = sigmoid(X @ theta) # m*n * n*1 = m*1\n",
    "    cost = (-1/m) * np.sum(y.T @ (np.log(h)) +  (1 - y).T @ (np.log(1 - h))  )# np.sum() is used to extract element in the 1*1 array\n",
    "    reg_cost = (lambda_ / (2 * m)) * np.sum(np.square(theta[1:]))\n",
    "    return cost + reg_cost\n",
    "\n",
    "X = data['X']\n",
    "# Add intercept term\n",
    "X = np.hstack((np.ones((X.shape[0], 1)), X)) # hstack水平叠加，np.ones创建全一矩阵\n",
    "# X_with_bias = np.insert(X, 0, 1, axis=1)  # 在第0列插入全1\n",
    "\n",
    "y = data['y']\n",
    "theta = np.zeros(X.shape[1])\n",
    "lambda_ = 1.0\n",
    "cost = RegularizedCostFunction(theta, X, y, lambda_)\n",
    "print(f\"Regularized Cost: {cost}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfabe3af",
   "metadata": {},
   "source": [
    "$$\\theta_0:=\\theta_0-\\alpha\\frac{1}{m}\\sum_{i=1}^{m}{\\left[f_{\\vec{w},b}\\left({\\vec{x}}^{\\left(i\\right)}\\right)-y^{\\left(i\\right)}\\right]x_0^{\\left(i\\right)}}$$\n",
    "$$\\theta_j:=\\theta_j-\\alpha\\frac{1}{m}\\sum_{i=1}^{m}{f_{\\vec{w},b}\\left[\\left({\\vec{x}}^{\\left(i\\right)}\\right)-y^{\\left(i\\right)}\\right]x_j^{\\left(i\\right)}}+\\alpha\\frac{\\lambda}{m}\\theta_j$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "e0a6e922",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RegularizedGradientDescent_auto(theta:np.ndarray, X:np.ndarray, y:np.ndarray, initial_alpha:float, lambda_:float, max_iter:int, tolerance: float) -> list:\n",
    "    m = len(y)\n",
    "    theta = theta.reshape(-1, 1)\n",
    "    y = y.reshape(-1, 1)\n",
    "\n",
    "    pre_cost = float('inf')\n",
    "    for i in range(max_iter):\n",
    "        error = sigmoid(X @ theta) - y # m*n * n*1 = m*1\n",
    "        gradient = (1/m) * (X * error)\n",
    "        gradient = np.sum(gradient, axis=0).reshape(-1, 1)\n",
    "        theta[0] -= initial_alpha * gradient[0] \n",
    "        for i in range(1, len(theta)):\n",
    "            theta[i] -= initial_alpha * (gradient[i] - (lambda_ / m) * theta[i])\n",
    "        cur_cost=(RegularizedCostFunction(theta, X, y, lambda_))\n",
    "        if abs(pre_cost - cur_cost) < tolerance :\n",
    "            break\n",
    "        if pre_cost < cur_cost:\n",
    "            initial_alpha *= 0.5\n",
    "        else:\n",
    "            initial_alpha *= 1.1\n",
    "        pre_cost = cur_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "e1e74357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10],\n",
       "       [10],\n",
       "       [10],\n",
       "       ...,\n",
       "       [ 9],\n",
       "       [ 9],\n",
       "       [ 9]], dtype=uint8)"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "5069461a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def OneVsAll(X:np.ndarray, y:np.ndarray, num_labels:int, initial_alpha:float, lambda_:float, max_iter:int, tolerance: float) -> np.ndarray:\n",
    "    m, n = X.shape\n",
    "    all_theta = np.zeros((num_labels, n))\n",
    "    \n",
    "    for c in range(num_labels):\n",
    "        # binary_y = y == (c+1)\n",
    "        binary_y = np.where(y == (c + 1) , 1, 0)\n",
    "        initial_theta = np.zeros(n)\n",
    "        RegularizedGradientDescent_auto(initial_theta, X, binary_y, initial_alpha, lambda_, max_iter, tolerance)\n",
    "        all_theta[c] = initial_theta\n",
    "\n",
    "    return all_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "1e1a4dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_theta = OneVsAll(X, y, 10, 1, 0.1, 10000, 1e-20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "7636ea44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.10098247e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "         5.30906348e-03,  3.13494912e-07,  0.00000000e+00],\n",
       "       [-3.84094003e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "         1.27338522e-02, -1.45420572e-03,  0.00000000e+00],\n",
       "       [-5.93540881e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "        -8.11047350e-05, -2.50675186e-07,  0.00000000e+00],\n",
       "       ...,\n",
       "       [-9.41759719e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "        -4.06482449e-04,  3.90034602e-05,  0.00000000e+00],\n",
       "       [-5.73952453e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "        -6.68533036e-03,  5.29220024e-04,  0.00000000e+00],\n",
       "       [-6.87515205e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "        -4.19337995e-04,  1.97398919e-05,  0.00000000e+00]])"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "254223fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_all(X, all_theta):\n",
    "    \n",
    "    # convert to matrices\n",
    "    X = np.matrix(X)\n",
    "    all_theta = np.matrix(all_theta)\n",
    "    \n",
    "    # compute the class probability for each class on each training instance\n",
    "    h = sigmoid(X * all_theta.T) # m*n * n*k = m*k\n",
    "    \n",
    "    # create array of the index with the maximum probability\n",
    "    h_argmax = np.argmax(h, axis=1)\n",
    "    print(h_argmax)\n",
    "    \n",
    "    # because our array was zero-indexed we need to add one for the true label prediction\n",
    "    h_argmax = h_argmax + 1\n",
    "    \n",
    "    return h_argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "669a1ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9]\n",
      " [9]\n",
      " [9]\n",
      " ...\n",
      " [8]\n",
      " [8]\n",
      " [8]]\n",
      "accuracy = 96.24000000000001%\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict_all(X, all_theta)\n",
    "correct = [1 if a == b else 0 for (a, b) in zip(y_pred, data['y'])]\n",
    "accuracy = (sum(map(int, correct)) / float(len(correct)))\n",
    "print ('accuracy = {0}%'.format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "46424e49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.41143500e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "         5.93055358e-04,  1.79122523e-07,  0.00000000e+00],\n",
       "       [-2.64020469e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "         2.36424484e-03, -2.71934725e-04,  0.00000000e+00],\n",
       "       [-4.09366113e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "        -2.43230235e-05,  1.66804888e-08,  0.00000000e+00],\n",
       "       ...,\n",
       "       [-7.28614575e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "        -1.23158261e-04,  1.13727980e-05,  0.00000000e+00],\n",
       "       [-4.09295473e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "        -4.01060495e-04,  2.42297992e-05,  0.00000000e+00],\n",
       "       [-3.79469762e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "        -3.59416326e-04,  1.09752741e-05,  0.00000000e+00]])"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_theta2 = OneVsAll(X, y, 10, 0.1, 1, 10000, 1e-8)\n",
    "all_theta2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "81999f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9]\n",
      " [9]\n",
      " [9]\n",
      " ...\n",
      " [8]\n",
      " [8]\n",
      " [6]]\n",
      "accuracy = 93.96%\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict_all(X, all_theta2)\n",
    "correct = [1 if a == b else 0 for (a, b) in zip(y_pred, data['y'])]\n",
    "accuracy = (sum(map(int, correct)) / float(len(correct)))\n",
    "print ('accuracy = {0}%'.format(accuracy * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
